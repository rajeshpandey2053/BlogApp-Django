{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the datasets and preprocessing the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 08:26:11.141594: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-14 08:26:11.141619: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /=255\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "x_train1 = x_train[0:1000]\n",
    "y_train1 = y_train[0:1000]\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0],28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_batches = [x_train1[k:k+32] for k in range(0, 1000, 32)]\n",
    "# (mini_batches[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head1, *tail1 = y_train\n",
    "# head1.size\n",
    "# head, *tail = x_train\n",
    "# head1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Neural Nets and Activation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNets:\n",
    "    def __init__(self,in_dim, out_dim):\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.w = np.random.randn(out_dim, in_dim) / np.sqrt(in_dim + out_dim)\n",
    "        self.b = np.random.randn(out_dim,1) / np.sqrt(in_dim + out_dim)\n",
    "        \n",
    "    def propagate_forward(self, x_train):\n",
    "        \"\"\" forward propagating the inputs \"\"\"\n",
    "        self.x_train = x_train\n",
    "        return np.dot(self.w,x_train) + self.b\n",
    "    \n",
    "    def propagate_backward(self, output_loss, mini_batche_y, l_rate):\n",
    "        dW = np.dot(output_loss, self.x_train.T)\n",
    "        dB = np.array(np.sum(output_loss, axis=1).reshape((output_loss.shape[0],1)))\n",
    "#         dB = output_loss\n",
    "#         dB = np.tile(output_loss, (1,len(mini_batche_y[0])))\n",
    "#         dB = np.sum(output_loss, axis = 0)\n",
    "#         print(dB.shape)\n",
    "        self.w -= (l_rate/len(mini_batche_y[1])) * dW\n",
    "        self.b -= (l_rate/len(mini_batche_y[1])) * dB\n",
    "        input_loss = np.dot(self.w.T, output_loss)\n",
    "        return input_loss\n",
    "    \n",
    "class SquashingLayer:\n",
    "    def __init__(self, squashing_func, squashing_derivative_func):\n",
    "        self.squashing_func = squashing_func\n",
    "        self.squashing_derivative_func = squashing_derivative_func\n",
    "        \n",
    "    def propagate_forward(self, x_train):\n",
    "        self.x_train = x_train\n",
    "        return self.squashing_func(self.x_train)\n",
    "    \n",
    "    def propagate_backward(self, output_loss,mini_batche_y, l_rate):\n",
    "        return np.multiply(output_loss,self.squashing_derivative_func(self.x_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Final Output Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalSoftmaxLayer:\n",
    "    def __init__(self, in_dim):\n",
    "        self.in_dim = in_dim\n",
    "    \n",
    "    def propagate_forward(self, x_train):\n",
    "        self.output = np.exp(x_train) / np.sum(np.exp(x_train), axis=0)\n",
    "        return self.output\n",
    "    \n",
    "    def propagate_backward(self, predicted_output,mini_batche_y, l_rate):\n",
    "        return predicted_output - mini_batche_y\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations Functions and its Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defintions of squashing functions and its derivatives\n",
    "\n",
    "def relu_func(input_x):\n",
    "    return np.maximum(input_x,0)\n",
    "\n",
    "def relu_derivative_func(input_x):\n",
    "    return np.array(input_x >= 0).astype('int')\n",
    "\n",
    "def sigmoid_func(input_x):\n",
    "    return 1.0 / (1.0 + np.exp(-input_x))\n",
    "\n",
    "def sigmoid_derivative_func(z): \n",
    "    return sigmoid_func(z)*(1-sigmoid_func(z))\n",
    "\n",
    "\n",
    "#     def softmax(self, x, derivative=False):\n",
    "#         # Numerically stable with large exponentials\n",
    "#         exps = np.exp(x - x.max())\n",
    "#         if derivative:\n",
    "#             return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "#         return exps / np.sum(exps, axis=0)\n",
    "\n",
    "# def softmax_func(x_train):\n",
    "#     output = np.exp(x_train) / np.sum(np.exp(x_train), axis=0)\n",
    "#     return self.output\n",
    "\n",
    "def softmax_func(x):\n",
    "    exps = np.exp(x - x.max())\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "\n",
    "def softmax_derivative_func(x):\n",
    "    return 1\n",
    "#     exps = np.exp(x - x.max())\n",
    "#     return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions and its derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions of Loss functions and its derivatives\n",
    "\n",
    "def mean_squared_error(actual_output, predicted_output):\n",
    "    return np.mean(np.power(actual_output - predicted_output, 2))\n",
    "\n",
    "def derivative_mean_squared(actual_output, predicted_output):\n",
    "    return 2 * (actual_output - predicted_output) / predicted_output.size\n",
    "\n",
    "# def softmax_loss()\n",
    "def compute_loss(X, Y):\n",
    "\n",
    "    L_sum = np.sum(np.multiply(X, np.log(Y)))\n",
    "    m = X.shape[1]\n",
    "    L = -(1/m) * L_sum\n",
    "\n",
    "    return L\n",
    "\n",
    "def derivative_softmax_loss(actual_output, predicted_output):\n",
    "    return predicted_output - actual_output\n",
    "#     return 2 * (predicted_output - actual_output) / predicted_output.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking layers and training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,32) (128,32) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#         error = derivative_softmax_loss(mini_batche_y, output)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(network):\n\u001b[0;32m---> 31\u001b[0m                     error \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_batche_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43ml_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# calculate average error on all samples\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     err \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m samples\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mSquashingLayer.propagate_backward\u001b[0;34m(self, output_loss, mini_batche_y, l_rate)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpropagate_backward\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_loss,mini_batche_y, l_rate):\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquashing_derivative_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,32) (128,32) "
     ]
    }
   ],
   "source": [
    "network = [\n",
    "    NeuralNets(28*28, 128),\n",
    "    SquashingLayer(relu_func, relu_derivative_func),\n",
    "    NeuralNets(128,10),\n",
    "    SquashingLayer(sigmoid_func, sigmoid_derivative_func),\n",
    "    FinalSoftmaxLayer(10)\n",
    "    \n",
    "]\n",
    "\n",
    "no_of_epochs = 40\n",
    "l_rate = 3.0\n",
    "mini_batch_size = 32\n",
    "#training\n",
    "samples = len(x_train1)\n",
    "for epoch in range(no_of_epochs):\n",
    "#     random.shuffle(x_train1)\n",
    "    err = 0\n",
    "    for k in range(0, samples, mini_batch_size):\n",
    "        # forward propagation\n",
    "        mini_batche_x = x_train1[k:k+mini_batch_size].T\n",
    "        mini_batche_y = y_train1[k:k+mini_batch_size].T\n",
    "        output = mini_batche_x\n",
    "        for layer in network:\n",
    "            output = layer.propagate_forward(output)\n",
    "        \n",
    "#         err += mean_squared_error(mini_batche_y, output)\n",
    "        err += compute_loss(mini_batche_y, output)\n",
    "        \n",
    "#         error = derivative_softmax_loss(mini_batche_y, output)\n",
    "        for layer in reversed(network):\n",
    "                    error = layer.propagate_backward(output, mini_batche_y,l_rate)\n",
    "        \n",
    "        # calculate average error on all samples\n",
    "    err /= samples\n",
    "    print('epoch %d/%d   error=%f' % (epoch+1, no_of_epochs, err))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# a=np.array([[1,2],[1,2]])\n",
    "# b=np.sum(a, axis = 0)\n",
    "# b\n",
    "# nabla_B = [np.tile(np.zeros(b.shape), (1, self.mini_batch_size))\n",
    "#                 for b in self.biases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
